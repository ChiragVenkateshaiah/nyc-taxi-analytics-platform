{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62db7442-ea91-41b7-8983-968aebb7cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî HADOOP_HOME set to:, os.environ['HADOOP_HOME']\n",
      "‚úî Added to PATH: C:\\hadoop\\bin\n",
      "\n",
      " ‚úî winutils.exe: True\n",
      " ‚úî hadoop.dll: True\n",
      "\n",
      "üéâ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from utils.spark_session import get_spark_session\n",
    "from utils.hadoop_setup import complete_hadoop_setup\n",
    "\n",
    "# Hadoop setup run\n",
    "complete_hadoop_setup()\n",
    "\n",
    "\n",
    "# Create Spark Session and assign it to spark 'variable'\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "731565ff-fb53-47bb-ae23-071bf7a22965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    current_date,\n",
    "    current_timestamp,\n",
    "    input_file_name,\n",
    "    lit\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "# Project configuration\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\chira\\Desktop\\data_engineering\\PySpark\\nyc-taxi-analytics-platform\")\n",
    "\n",
    "# Base paths\n",
    "LANDING_BASE_PATH = PROJECT_ROOT / \"data\" / \"landing\" / \"nyc_taxi\"\n",
    "BRONZE_BASE_PATH = PROJECT_ROOT / \"data\" / \"bronze\" / \"nyc_taxi\"\n",
    "\n",
    "# Process only green taxi\n",
    "TAXI_TYPE = \"green\"\n",
    "\n",
    "# Batch ID for auditability\n",
    "BATCH_ID = f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "PROCESSED_FILES_PATH = str(PROJECT_ROOT / \"data\" / \"bronze\" / \"_processed_files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "992cb551-006d-4c26-ba5a-c24a249485d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û° Loaded 1 previously processed files\n"
     ]
    }
   ],
   "source": [
    "# Initialize processed files tracker if needed\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "processed_schema = StructType([\n",
    "    StructField(\"file_path\", StringType(), False),\n",
    "    StructField(\"processed_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Robust approach: Try to read, create if it fails\n",
    "try:\n",
    "    processed_files_df = spark.read.format(\"delta\").load(PROCESSED_FILES_PATH)\n",
    "    processed_files = {row.file_path for row in processed_files_df.collect()}\n",
    "    print(f\"‚û° Loaded {len(processed_files)} previously processed files\")\n",
    "except Exception as e:\n",
    "    print(\"‚û° Creating new _processed_files tracker table...\")\n",
    "    # Ensure directory exists and is empty\n",
    "    processed_files_path_obj = Path(PROCESSED_FILES_PATH)\n",
    "    if processed_files_path_obj.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(processed_files_path_obj)\n",
    "        print(\" ‚úî Cleaned up existing directory\")\n",
    "\n",
    "    # Create new Delta table\n",
    "    spark.createDataFrame([], processed_schema) \\\n",
    "        .write.format(\"delta\") \\\n",
    "        .save(PROCESSED_FILES_PATH)\n",
    "\n",
    "    processed_files = set()\n",
    "    print(\" ‚úî Tracker table created (empty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fe5861d-2999-4b90-a0f0-7798560a2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_green_to_bronze():\n",
    "    \"\"\"Ingest green taxi data from landing to bronze layer\"\"\"\n",
    "    \n",
    "    taxi_type = TAXI_TYPE\n",
    "    landing_path = LANDING_BASE_PATH / taxi_type\n",
    "    bronze_path = BRONZE_BASE_PATH / taxi_type\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {taxi_type.upper()} taxi data\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not landing_path.exists():\n",
    "        print(f\"‚ùå No landing data for {taxi_type}\")\n",
    "        return\n",
    "\n",
    "    # Only process 2025 data\n",
    "    year_2025_path = landing_path / \"2025\"\n",
    "\n",
    "    if not year_2025_path.exists():\n",
    "        print(f\"‚ùå No 2025 directory found for {taxi_type}\")\n",
    "        return\n",
    "\n",
    "    # Check for valid parquet files\n",
    "    parquet_files = list(year_2025_path.rglob(\"*.parquet\"))\n",
    "\n",
    "    # Filter out empty files (0 bytes)\n",
    "    valid_files = [f for f in parquet_files if f.stat().st_size > 0]\n",
    "\n",
    "    if not valid_files:\n",
    "        print(f\"‚ö† No valid parquet files found in 2025 directory\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"‚û° Reading {taxi_type} data for year 2025...\")\n",
    "        print(f\"‚û° Found {len(valid_files)} valid parquet file(s)\")\n",
    "\n",
    "        # Display file details\n",
    "        for pf in valid_files:\n",
    "            file_size_mb = pf.stat().st_size / (1024 * 1024)\n",
    "            print(f\"    -{pf.parent.name}/{pf.name}: {file_size_mb:.2f} MB\")\n",
    "\n",
    "        year_path_str = str(year_2025_path).replace('\\\\', '/')\n",
    "\n",
    "        df = spark.read \\\n",
    "            .option(\"basePath\", year_path_str) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .parquet(year_path_str)\n",
    "\n",
    "        row_count = df.count()\n",
    "        print(f\" ‚úî Loaded {row_count:,} total rows from 2025\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ‚ùå Could not read 2025 data: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Filter already processed files\n",
    "    print(f\" ‚û° Checking for already processed files...\")\n",
    "    df = df.withColumn(\"_source_file\", input_file_name())\n",
    "    df_new = df.filter(~df[\"_source_file\"].isin(processed_files))\n",
    "\n",
    "    new_count = df_new.count()\n",
    "    if new_count == 0:\n",
    "        print(f\" ‚úî No new files to ingest for {taxi_type}\")\n",
    "        return\n",
    "\n",
    "    print(f\" ‚û° Found {new_count:,} new records to process...\")\n",
    "\n",
    "    # Add metadata columns\n",
    "    df_bronze = (\n",
    "        df_new\n",
    "        .withColumn(\"_ingestion_date\", current_date())\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_batch_id\", lit(BATCH_ID))\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Ensure bronze path exists\n",
    "    bronze_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Write to bronze Delta Lake\n",
    "    bronze_path_str = str(bronze_path).replace('\\\\', '/')\n",
    "    print(f\" ‚û° Writing to bronze layer: {bronze_path_str}\")\n",
    "\n",
    "    df_bronze.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(bronze_path_str)\n",
    "\n",
    "    # Update processed files tracker\n",
    "    new_files_df = df_new.select(\"_source_file\") \\\n",
    "        .distinct() \\\n",
    "        .withColumnRenamed(\"_source_file\", \"file_path\") \\\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "    files_to_track = new_files_df.count()\n",
    "    new_files_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(PROCESSED_FILES_PATH)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" ‚úî Bronze ingestion completed for {taxi_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Total records written: {new_count:,}\")\n",
    "    print(f\"   Files tracked: {files_to_track}\")\n",
    "    print(f\"   Batch ID: {BATCH_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33a63bca-5e6a-4343-a0d0-2cb189eb9a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "Starting GREEN Taxi Bronze Layer Ingestion\n",
      "Batch: batch_20260209_200002\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "Processing GREEN taxi data\n",
      "============================================================\n",
      "‚û° Reading green data for year 2025...\n",
      "‚û° Found 1 valid parquet file(s)\n",
      "    -month=09/green_tripdata_2025-09.parquet: 1.15 MB\n",
      " ‚úî Loaded 48,893 total rows from 2025\n",
      " ‚û° Checking for already processed files...\n",
      " ‚úî No new files to ingest for green\n",
      "\n",
      " ‚úî SUCCESS: Green taxi data ingestion completed!\n",
      "\n",
      "############################################################\n",
      "Process Complete\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "# Execute ingestion\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"Starting GREEN Taxi Bronze Layer Ingestion\")\n",
    "print(f\"Batch: {BATCH_ID}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "try:\n",
    "    ingest_green_to_bronze()\n",
    "    print(f\"\\n ‚úî SUCCESS: Green taxi data ingestion completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n ‚ùå ERROR: Green taxi ingestion failed\")\n",
    "    print(f\"ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"Process Complete\")\n",
    "print(f\"{'#'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc8553-ce5e-4cec-a654-d28e8694eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Conda)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
