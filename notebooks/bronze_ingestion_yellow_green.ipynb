{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7c2ef3-e4e0-4fdd-a273-98f2ed507658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî HADOOP_HOME set to:, os.environ['HADOOP_HOME']\n",
      "‚úî Added to PATH: C:\\hadoop\\bin\n",
      "\n",
      " ‚úî winutils.exe: True\n",
      " ‚úî hadoop.dll: True\n",
      "\n",
      "üéâ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from utils.spark_session import get_spark_session\n",
    "from utils.hadoop_setup import complete_hadoop_setup\n",
    "\n",
    "# Hadoop setup run\n",
    "complete_hadoop_setup()\n",
    "\n",
    "# Create Spark Session and assign it to spark 'Variable'\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc9cd22-635b-4c91-b327-1404a4481982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    current_timestamp,\n",
    "    current_date,\n",
    "    input_file_name,\n",
    "    lit\n",
    ")\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# Project Configuration\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\chira\\Desktop\\data_engineering\\PySpark\\nyc-taxi-analytics-platform\")\n",
    "\n",
    "\n",
    "# Base paths\n",
    "LANDING_BASE_PATH = PROJECT_ROOT /  \"data\" / \"landing\" / \"nyc_taxi\"\n",
    "BRONZE_BASE_PATH = PROJECT_ROOT / \"data\" / \"bronze\" / \"nyc_taxi\"\n",
    "\n",
    "\n",
    "# Taxi types to process\n",
    "TAXI_TYPES = [\"green\", \"yellow\"]\n",
    "\n",
    "\n",
    "# Batch ID for auditability\n",
    "BATCH_ID = f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "PROCESSED_FILES_PATH = str(PROJECT_ROOT / \"data\" / \"_processed_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ba08d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û° Loaded 2 previously processed files\n"
     ]
    }
   ],
   "source": [
    "#------ Processed-files tracker-------------#\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "processed_schema = StructType([\n",
    "    StructField(\"file_path\", StringType(), False),\n",
    "    StructField(\"processed_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "try:\n",
    "    processed_files_df = spark.read.format(\"delta\").load(PROCESSED_FILES_PATH)\n",
    "    processed_files = {row.file_path for row in processed_files_df.collect()}\n",
    "    print(f\"‚û° Loaded {len(processed_files)} previously processed files\")\n",
    "except Exception:\n",
    "    print(\"‚û° Creating new _processed_files tracker table...\")\n",
    "    processed_files_path_obj = Path(PROCESSED_FILES_PATH)\n",
    "    if processed_files_path_obj.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(processed_files_path_obj)\n",
    "        print(\" ‚úî Cleaned up existing directory\")\n",
    "    spark.createDataFrame([], processed_schema) \\\n",
    "         .write.format(\"delta\") \\\n",
    "         .save(PROCESSED_FILES_PATH)\n",
    "    processed_files = set()\n",
    "    print(\" ‚úî Tracker table created (empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b892ca",
   "metadata": {},
   "source": [
    "## Generic Ingestion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861b60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_to_bronze(taxi_type: str):\n",
    "    \"\"\"\n",
    "    Ingest one taxi type (green | yellow) from landing to bronze layer.\n",
    "    taxi_type must match the folder name under landing/nyc_taxi/.\n",
    "    \"\"\"\n",
    "    landing_path = LANDING_BASE_PATH / taxi_type\n",
    "    bronze_path = BRONZE_BASE_PATH / taxi_type\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {taxi_type.upper()} taxi data\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "    if not landing_path.exists():\n",
    "        print(f\"‚ùå No landing data found for {taxi_type} at: {landing_path}\")\n",
    "        return\n",
    "    \n",
    "    # Only process 2025 data\n",
    "    year_2025_path = landing_path / \"2025\"\n",
    "    if not year_2025_path.exists():\n",
    "        print(f\"‚ùå No 2025 directory found for {taxi_type}\")\n",
    "        return\n",
    "    \n",
    "    # Discover valid (non-empty) parquet files\n",
    "    parquet_files = list(year_2025_path.rglob(\"*.parquet\"))\n",
    "    valid_files = [f for f in parquet_files if f.stat().st_size > 0]\n",
    "\n",
    "    if not valid_files:\n",
    "        print(f\"‚ùå No valid parquet files found in: {year_2025_path}\")\n",
    "        return\n",
    "    \n",
    "    # ---------------Read--------------------\n",
    "\n",
    "    try:\n",
    "        print(f\"‚û° Reading {taxi_type} data for year 2025...\")\n",
    "        print(f\"‚û° Found {len(valid_files)} valid parquet file(s)\")\n",
    "        for pf in valid_files:\n",
    "            file_size_mb = pf.stat().st_size / (1024 * 1024)\n",
    "            print(f\"    - {pf.parent.name}/{pf.name}: {file_size_mb:.2f} MB\")\n",
    "\n",
    "        year_path_str = str(year_2025_path).replace(\"\\\\\", \"/\")\n",
    "        df = (\n",
    "            spark.read\n",
    "                 .option(\"basePath\", year_path_str)\n",
    "                 .option(\"mergeSchema\", \"true\")\n",
    "                 .parquet(year_path_str)\n",
    "        )\n",
    "        print(f\"‚úî Loaded {df.count():,} total rows from 2025\")\n",
    "    except Exception as e:\n",
    "        print(f\" ‚ùå Could not read 2025 data: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "\n",
    "    # ----------- Incremental Filter ------------------------\n",
    "    print(f\" ‚û° Checking for already-processed files...\")\n",
    "    df = df.withColumn(\"_source_file\", input_file_name())\n",
    "    df_new = df.filter(~df[\"_source_file\"].isin(processed_files))\n",
    "    new_count = df_new.count()\n",
    "\n",
    "    if new_count == 0:\n",
    "        print(f\" ‚úî No new records to ingest for {taxi_type} - already up to date\")\n",
    "        return\n",
    "    \n",
    "    print(f\" ‚û° Found {new_count:,} new records to process...\")\n",
    "\n",
    "\n",
    "    # -----------Add audit metadata -------------------------\n",
    "    df_bronze = (\n",
    "        df_new\n",
    "        .withColumn(\"_ingestion_date\", current_date())\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"BATCH_ID\", lit(BATCH_ID))\n",
    "    )\n",
    "\n",
    "    # ----------------Write to bronze Delta Lake ---------------------\n",
    "    new_files_df = (\n",
    "        df_new.select(\"_source_file\")\n",
    "              .distinct()\n",
    "              .withColumnRenamed(\"_source_file\", \"file_path\")\n",
    "              .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "    files_to_track = new_files_df.count()\n",
    "    new_files_df.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(PROCESSED_FILES_PATH)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" ‚úî Bronze ingestion completed for {taxi_type.upper()}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"    Total records written : {new_count:,}\")\n",
    "    print(f\"    Files tracked : {files_to_track}\")\n",
    "    print(f\"    BATCH ID : {BATCH_ID}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf064a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Convenience Wrapper--------------\n",
    "\n",
    "def ingest_green_to_bronze():\n",
    "    ingest_to_bronze(\"green\")\n",
    "\n",
    "\n",
    "def ingest_yellow_to_bronze():\n",
    "    ingest_to_bronze(\"yellow\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fec2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "Starting NYC Taxi Bronze Layer Ingestion\n",
      "Batch: batch_20260220_142523\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "Processing GREEN taxi data\n",
      "\n",
      "============================================================\n",
      "‚û° Reading green data for year 2025...\n",
      "‚û° Found 1 valid parquet file(s)\n",
      "    - month=09/green_tripdata_2025-09.parquet: 1.15 MB\n",
      "‚úî Loaded 48,893 total rows from 2025\n",
      " ‚û° Checking for already-processed files...\n",
      " ‚úî No new records to ingest for green - already up to date\n",
      "\n",
      " ‚úî SUCCESS: GREEN taxi data ingestion completed!\n",
      "\n",
      "============================================================\n",
      "Processing YELLOW taxi data\n",
      "\n",
      "============================================================\n",
      "‚û° Reading yellow data for year 2025...\n",
      "‚û° Found 1 valid parquet file(s)\n",
      "    - month=09/yellow_tripdata_2025-09.parquet: 69.08 MB\n",
      "‚úî Loaded 4,251,015 total rows from 2025\n",
      " ‚û° Checking for already-processed files...\n",
      " ‚úî No new records to ingest for yellow - already up to date\n",
      "\n",
      " ‚úî SUCCESS: YELLOW taxi data ingestion completed!\n",
      "\n",
      "############################################################\n",
      "Process complete - Summary\n",
      "\n",
      "############################################################\n",
      " ‚úî GREEN: SUCCESS\n",
      " ‚úî YELLOW: SUCCESS\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "# -----------Execute ingestion for all taxi types -------------\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"Starting NYC Taxi Bronze Layer Ingestion\")\n",
    "print(f\"Batch: {BATCH_ID}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "\n",
    "ingestion_tasks = [\n",
    "    (\"GREEN\", ingest_green_to_bronze),\n",
    "    (\"YELLOW\", ingest_yellow_to_bronze),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "for taxi_type_label, ingest_fn in ingestion_tasks:\n",
    "    try:\n",
    "        ingest_fn()\n",
    "        results[taxi_type_label] = \"SUCCESS\"\n",
    "        print(f\"\\n ‚úî SUCCESS: {taxi_type_label} taxi data ingestion completed!\")\n",
    "    except Exception as e:\n",
    "        results[taxi_type_label] = \"FAILED\"\n",
    "        print(f\"\\n ‚ùå ERROR: {taxi_type_label} taxi ingestion failed\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# --------Final Summary----------\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"Process complete - Summary\")\n",
    "print(f\"\\n{'#'*60}\")\n",
    "\n",
    "for taxi_type_label, status in results.items():\n",
    "    icon= \"‚úî\" if status == \"SUCCESS\" else \"‚ùå\"\n",
    "    print(f\" {icon} {taxi_type_label}: {status}\" )\n",
    "print(f\"{'#'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0133d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
