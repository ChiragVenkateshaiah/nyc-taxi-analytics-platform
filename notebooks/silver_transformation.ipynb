{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da6da28-24dc-4f1b-add4-3ddae7aa4423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” HADOOP_HOME set to:, os.environ['HADOOP_HOME']\n",
      "âœ” Added to PATH: C:\\hadoop\\bin\n",
      "\n",
      " âœ” winutils.exe: True\n",
      " âœ” hadoop.dll: True\n",
      "\n",
      "ðŸŽ‰ Setup complete!\n",
      "âœ” HADOOP_HOME set to:, os.environ['HADOOP_HOME']\n",
      "âœ” Added to PATH: C:\\hadoop\\bin\n",
      "\n",
      " âœ” winutils.exe: True\n",
      " âœ” hadoop.dll: True\n",
      "\n",
      "ðŸŽ‰ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from utils.spark_session import get_spark_session\n",
    "from utils.hadoop_setup import complete_hadoop_setup\n",
    "\n",
    "# Hadoop complete setup\n",
    "complete_hadoop_setup()\n",
    "\n",
    "# Create Spark Session and assign to spark 'variable'\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe781c28-bd90-49b8-a04c-e9379140d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\chira\\Desktop\\data_engineering\\PySpark\\nyc-taxi-analytics-platform\")\n",
    "\n",
    "BRONZE_GREEN_PATH = str(PROJECT_ROOT / \"data\" / \"bronze\" / \"nyc_taxi\" / \"green\")\n",
    "SILVER_GREEN_PATH = str(PROJECT_ROOT / \"data\" / \"silver\" / \"nyc_taxi\" / \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d05b41-a309-428b-b577-ffec7049c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze row count: 48,893\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- _source_file: string (nullable = true)\n",
      " |-- _ingestion_date: date (nullable = true)\n",
      " |-- _ingestion_timestamp: timestamp (nullable = true)\n",
      " |-- _batch_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze = spark.read.format(\"delta\").load(BRONZE_GREEN_PATH)\n",
    "\n",
    "print(f\"Bronze row count: {df_bronze.count():,}\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e78c80-5004-4c67-af03-d47887ab0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_selected = (\n",
    "    df_bronze\n",
    "    .select(\n",
    "        col(\"VendorID\").alias(\"vendor_id\"),\n",
    "        col(\"lpep_pickup_datetime\").alias(\"pickup_ts\"),\n",
    "        col(\"lpep_dropoff_datetime\").alias(\"dropoff_ts\"),\n",
    "        col(\"passenger_count\"),\n",
    "        col(\"trip_distance\"),\n",
    "        col(\"fare_amount\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"_ingestion_date\"),\n",
    "        col(\"_ingestion_timestamp\"),\n",
    "        col(\"_batch_id\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3791af4-910c-4aec-9d30-447db04a623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_valid = df_selected.filter(\n",
    "    (col(\"pickup_ts\").isNotNull()) &\n",
    "    (col(\"dropoff_ts\").isNotNull()) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") >= 0) &\n",
    "    (col(\"total_amount\") >= col(\"fare_amount\"))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8f36f0-f912-4597-8d7f-810a1d103f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 48,893\n",
      "Valid records: 47,364\n",
      "Dropped records: 1,529\n"
     ]
    }
   ],
   "source": [
    "valid_count = df_valid.count()\n",
    "total_count = df_selected.count()\n",
    "\n",
    "print(f\"Total records: {total_count:,}\")\n",
    "print(f\"Valid records: {valid_count:,}\")\n",
    "print(f\"Dropped records: {total_count - valid_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22999bdf-8196-473b-a0ab-b459b8067bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_silver = (\n",
    "    df_valid\n",
    "    .withColumn(\"year\", year(\"pickup_ts\"))\n",
    "    .withColumn(\"month\", month(\"pickup_ts\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a5da9a-54ef-41c0-9fe1-8ff0c6d1b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Silver layer write completed\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(SILVER_GREEN_PATH)\n",
    ")\n",
    "\n",
    "print(\"âœ” Silver layer write completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e17bba6-8e15-4ae9-81a7-7ac9096f7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2025|    9|47333|\n",
      "|2025|    8|   17|\n",
      "|2025|   10|   14|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(SILVER_GREEN_PATH).groupBy(\"year\", \"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7b2d8-8eab-4e66-8b79-baecb5f6b8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Conda)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
